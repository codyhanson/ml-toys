{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Random Forests and Ensemble Learning\n",
    "Aggregate groups of predictors and use the \"wisdom of the crowd\".\n",
    "You often use Ensemble methods as a later stage refinement, once good initial models have been created.\n",
    "\n",
    "It is a good idea for each predictor in an ensemble to be as independent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn voting classifier, using moons dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=\n",
    "                             [('lr', log_clf),\n",
    "                             ('rf', rnd_clf), \n",
    "                             ('svc', svm_clf)from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "                             ], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.888\n",
      "VotingClassifier 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clh/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft voting uses models that provide probabilities, and the ensemble prediction is the class with the highest average probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "Instead of using different models in the ensemble, you can use different subsets of the training data.\n",
    "\n",
    "Bagging: sampling with replacement. (\"bootstrap aggregation\")\n",
    "Pasting: sampling without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging and Pasting in sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# bootstrap=False for Pasting mode.\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "# Automatically uses soft voting, if clasifier has predict_proba() method.\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out Of Bag evaluation (part of training set not sampled)\n",
    "# Built in using oob_score param.\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means it is likely to do about this well on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but lets check and compare.\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41104294, 0.58895706],\n",
       "       [0.37142857, 0.62857143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10526316, 0.89473684],\n",
       "       [0.33516484, 0.66483516],\n",
       "       [0.00990099, 0.99009901],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96354167, 0.03645833],\n",
       "       [0.78888889, 0.21111111],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.75520833, 0.24479167],\n",
       "       [0.82954545, 0.17045455],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.96774194, 0.03225806],\n",
       "       [0.99502488, 0.00497512],\n",
       "       [0.02906977, 0.97093023],\n",
       "       [0.36756757, 0.63243243],\n",
       "       [0.90419162, 0.09580838],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96428571, 0.03571429],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61666667, 0.38333333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13684211, 0.86315789],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18181818, 0.81818182],\n",
       "       [0.39010989, 0.60989011],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02926829, 0.97073171],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.98870056, 0.01129944],\n",
       "       [0.87878788, 0.12121212],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.97005988, 0.02994012],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04395604, 0.95604396],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.77073171, 0.22926829],\n",
       "       [0.38888889, 0.61111111],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.6424581 , 0.3575419 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.85945946, 0.14054054],\n",
       "       [1.        , 0.        ],\n",
       "       [0.61170213, 0.38829787],\n",
       "       [0.12432432, 0.87567568],\n",
       "       [0.65060241, 0.34939759],\n",
       "       [0.91616766, 0.08383234],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16      , 0.84      ],\n",
       "       [0.87292818, 0.12707182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05376344, 0.94623656],\n",
       "       [0.03553299, 0.96446701],\n",
       "       [0.29310345, 0.70689655],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.87292818, 0.12707182],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27027027, 0.72972973],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93478261, 0.06521739],\n",
       "       [0.83815029, 0.16184971],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14705882, 0.85294118],\n",
       "       [0.60555556, 0.39444444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06451613, 0.93548387],\n",
       "       [0.4640884 , 0.5359116 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.015625  , 0.984375  ],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.21621622, 0.78378378],\n",
       "       [0.48901099, 0.51098901],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.32022472, 0.67977528],\n",
       "       [0.8700565 , 0.1299435 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.75845411, 0.24154589],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94350282, 0.05649718],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.26630435, 0.73369565],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.27272727, 0.72727273],\n",
       "       [0.98395722, 0.01604278],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.7287234 , 0.2712766 ],\n",
       "       [0.39267016, 0.60732984],\n",
       "       [0.41954023, 0.58045977],\n",
       "       [0.89893617, 0.10106383],\n",
       "       [0.93467337, 0.06532663],\n",
       "       [0.04301075, 0.95698925],\n",
       "       [0.80927835, 0.19072165],\n",
       "       [0.01123596, 0.98876404],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.97142857, 0.02857143],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03351955, 0.96648045],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94805195, 0.05194805],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.        , 1.        ],\n",
       "       [0.31764706, 0.68235294],\n",
       "       [0.30285714, 0.69714286],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32941176, 0.67058824],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96808511, 0.03191489],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01570681, 0.98429319],\n",
       "       [0.65454545, 0.34545455],\n",
       "       [0.89411765, 0.10588235],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99382716, 0.00617284],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06666667, 0.93333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03225806, 0.96774194],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02673797, 0.97326203],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.95081967, 0.04918033],\n",
       "       [0.69035533, 0.30964467],\n",
       "       [0.63841808, 0.36158192],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15432099, 0.84567901],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95      , 0.05      ],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41011236, 0.58988764],\n",
       "       [0.81463415, 0.18536585],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0212766 , 0.9787234 ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.96825397, 0.03174603],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24858757, 0.75141243],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97916667, 0.02083333],\n",
       "       [0.81407035, 0.18592965],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09223301, 0.90776699],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02475248, 0.97524752],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06382979, 0.93617021],\n",
       "       [0.99411765, 0.00588235],\n",
       "       [0.77094972, 0.22905028],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.89893617, 0.10106383],\n",
       "       [0.98404255, 0.01595745],\n",
       "       [0.17283951, 0.82716049],\n",
       "       [0.19459459, 0.80540541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22513089, 0.77486911],\n",
       "       [0.9902439 , 0.0097561 ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48913043, 0.51086957],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05641026, 0.94358974],\n",
       "       [0.09625668, 0.90374332],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.02985075, 0.97014925],\n",
       "       [1.        , 0.        ],\n",
       "       [0.40322581, 0.59677419],\n",
       "       [0.0989011 , 0.9010989 ],\n",
       "       [0.57222222, 0.42777778],\n",
       "       [0.63541667, 0.36458333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.55113636, 0.44886364],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.19337017, 0.80662983],\n",
       "       [0.79775281, 0.20224719],\n",
       "       [0.07017544, 0.92982456],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83615819, 0.16384181],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00487805, 0.99512195],\n",
       "       [0.11917098, 0.88082902],\n",
       "       [0.01595745, 0.98404255],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93203883, 0.06796117],\n",
       "       [0.15625   , 0.84375   ],\n",
       "       [0.94708995, 0.05291005],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.60606061, 0.39393939],\n",
       "       [0.08205128, 0.91794872],\n",
       "       [0.98039216, 0.01960784],\n",
       "       [0.78865979, 0.21134021],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96703297, 0.03296703],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29189189, 0.70810811],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.85326087, 0.14673913],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75274725, 0.24725275],\n",
       "       [0.96132597, 0.03867403],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.53038674, 0.46961326],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9137931 , 0.0862069 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82887701, 0.17112299],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74456522, 0.25543478],\n",
       "       [0.08333333, 0.91666667],\n",
       "       [0.52849741, 0.47150259],\n",
       "       [0.21714286, 0.78285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83084577, 0.16915423],\n",
       "       [0.83060109, 0.16939891],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98404255, 0.01595745],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04324324, 0.95675676],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.93370166, 0.06629834],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5177665 , 0.4822335 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.02162162, 0.97837838],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9744898 , 0.0255102 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07462687, 0.92537313],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02247191, 0.97752809],\n",
       "       [1.        , 0.        ],\n",
       "       [0.16129032, 0.83870968],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33139535, 0.66860465],\n",
       "       [0.05583756, 0.94416244],\n",
       "       [0.22099448, 0.77900552],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.19512195, 0.80487805],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94886364, 0.05113636],\n",
       "       [0.24456522, 0.75543478],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.03954802, 0.96045198],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [0.68817204, 0.31182796]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oob decision function\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "You can also sample certain features of the input data, for even more diversity in your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Ensemble of decision trees, trained via bagging method, max_samples set to size of training set (i.e. sample entire set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=16,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#use all cpu cores.\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Random forests let you measure the relative importance of each input feature. Measured by how much tree nodes that use a feature reduce impurity (on average across all trees in forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09980536920905438\n",
      "sepal width (cm) 0.02270575768013286\n",
      "petal length (cm) 0.422175217711915\n",
      "petal width (cm) 0.45531365539889745\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows that sepal length and width are not that important of features. Random forests are a good way to get an idea of which features are important in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "(Hypothesis Boosting) is any ensemble method that can combine several weak learners into a strong learner.\n",
    "\n",
    "AdaBoost (Adaptive Boosting\n",
    "- Sequentially run models and for each data point they mis classify, increase the weight to better fit in the next round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)\n",
    "\n",
    "#train second tree on the residual errors made by first predictor\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)\n",
    "\n",
    "#Repeat for 3rd tree\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)\n",
    "\n",
    "#run the whole ensemble.\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use GradientBoostingRegressor for a built in version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "Instead of using a simple voting scheme with an ensemble, you can train another model to take in the outputs of the ensemble predictors, and then make the final prediction. (extra model is called blender, or meta learner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
